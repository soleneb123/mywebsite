{"title":"Forecasting the Number of Passengers on Geneva’s TPG Network per Time Slot","markdown":{"yaml":{"title":"Forecasting the Number of Passengers on Geneva’s TPG Network per Time Slot","authors":"Solène Berney, Nela Brack, Nils Stadler, Morgane Vuissoz","date":"January 10, 2025","format":"html","keywords":["Machine Learning","Data Science","Public Transport","Forecasting","Time Series"],"execute":{"eval":false,"freeze":true},"code-fold":true},"headingText":"1. Introduction and Motivation","containsRefs":false,"markdown":"\n\n\nTaking a bus, tram, or train is part of daily life, but the experience is always more enjoyable when one can find a seat and avoid overcrowding. Anticipating passenger demand is therefore a key challenge for both travelers and public transport operators.\n\nThis project develops a machine learning model to forecast the number of boarding passengers on Geneva’s public transport network (TPG) on an hourly basis. Such forecasts could help passengers choose less crowded travel times and support TPG in short-term capacity planning.\n\nWe focus on a network-wide prediction setting, capturing broad mobility patterns while respecting real-world forecasting constraints such as strong seasonality, non-linear demand, and structural breaks, such as COVID-19 period.\n\nThis work was completed as a group project in the Data Science Fundamentals (DSF) program at the University of St. Gallen.\n\n# 2. Methodology\n\n## Datasets cleaning\n\n### Core Passenger Data\n\nWe used the TPG ridership dataset, covering 2019 onwards with hourly counts for the whole network (\\~58,000 observations).\n\nData cleaning steps:\n\n-   Removed rows with missing or non-definitive values\n-   Excluded 2 and 3 am slots due to insufficient data\n-   One-hot encoded categorical variables such as day-of-week and schedule type\n\nFinal dataset: 54,098 rows × 61 columns.\n\n::: {.callout-note collapse=\"true\"}\n## TPG Dataset Cleaning\n``` python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"00_datasets/original/tpg.csv\", sep=\";\")\n\ndf = df.dropna(subset=[\"Time Slot\"])        # Remove rows with missing Time Slot\n\ndf = df[df[\"donnees_definitives\"] == True]  # Remove non-definitive data\n\ndf = df[df[\"Time Slot\"] != \"-\"]             # Remove invalid time slots\n\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])     # Parse date and time\ndf[\"Time Slot\"] = pd.to_numeric(df[\"Time Slot\"])\n\ndf = df[~df[\"Time Slot\"].isin([2, 3])]      # Drop 2 and 3 am observations\n\n# One-hot encoding of categorical variables\nweek_day_one_hot = pd.get_dummies(df[\"Day Week\"], prefix=\"day\")\nschedule_type_one_hot = pd.get_dummies(df[\"Schedule Type\"], prefix=\"schedule\")\nmonths_one_hot = pd.get_dummies(df[\"month\"], prefix=\"month\")\nyears_one_hot = pd.get_dummies(df[\"year\"], prefix=\"year\")\ntime_slots_one_hot = pd.get_dummies(df[\"Time Slot\"], prefix=\"slot\")\n\n# Combine all features\ndf_encoded = pd.concat([df, week_day_one_hot, schedule_type_one_hot, \n    months_one_hot, years_one_hot, time_slots_one_hot], axis=1)\n\n# Drop redundant and original categorical columns\ncolumns_to_drop = [\n    \"Day Week\",              # Encoded as day_\n    \"Schedule Type\",         # Encoded as schedule_  \n    \"Index Day Week\",        # Redundant with day_of_week\n    \"donnees_definitives\",   # All True after \"False\" values were removed\n    \"Week Index\"             # Not used in our predictive models\n]\n\ndf_encoded = df_encoded.drop(columns=columns_to_drop)\ndf_encoded = df_encoded.dropna().reset_index(drop=True) # drop any remaining NaN values\n\ndf_final = df_encoded.sort_values([\"Date\", \"Time Slot\"]).reset_index(drop=True)\n```\n:::\n\n### Weather Data\n\nWeather influences passenger behavior. We included daily average temperature, rainfall, and wind speed from a station 5 km from Geneva city center. To avoid leakage (i.e. to ensure that only information available at prediction time is used), weather features were lagged by one day.\n\n### Demographics and Abonnements\n\n-   Canton population data (yearly) to capture structural shifts,\n-   Counts of GA, Half-Fare, and Unireso subscriptions filtered to Geneva postal codes, aggregated yearly,\n-   2025 values extrapolated from 2024 growth rates\n\n::: {.callout-note collapse=\"true\"}\n## Abonnements Dataset Cleaning\n```\n# import data of halbtax and GA\ndf = pd.read_csv(\"00_datasets/original/generalabo_halbtax.csv\", sep=';')\n\n# Drop flag columns\ndf = df.drop(['GA_AG_flag', 'HTA_ADT_meta-prezzo_HFT_flag'], axis=1)\n\n# convert all of the values to integer\ndf[['Postal code', 'GA_AG', 'HTA_ADT_meta-prezzo_HFT']] = df[['Postal code', 'GA_AG', 'HTA_ADT_meta-prezzo_HFT']].astype(int)\n\n# select all rows which have a value between 1200 and 1299 in the Postal Code column (canton of Geneva)\ndf_geneva = df[df[\"Postal code\"].between(1200, 1299)]\n\n# sum them up by years\ndf_yearly = df_geneva.groupby('Jahr_An_Anno_Year')[['GA_AG', 'HTA_ADT_meta-prezzo_HFT']].sum().reset_index()\n\n# rename the features\ndf_yearly = df_yearly.rename(columns={\"Jahr_An_Anno_Year\": \"year\", \"GA_AG\": \"#GA\", \"HTA_ADT_meta-prezzo_HFT\": \"#half-fare\"})\n\n\n\n# import data of the CH dataset (all networks)\ndf_verbunde = pd.read_csv(\"00_datasets/original/verbunde.csv\", sep=';')\n\n# drop unnecessary columns\ndf_verbunde = df_verbunde.drop(['Flag'], axis=1)\n\n# select only the data that is acutally = unireso\ndf_unireso = df_verbunde[df_verbunde[\"Verbund_Communaute_Comunita_Network\"] == \"unireso\"]\n\n# sum them up per year again\ndf_unireso_yearly = df_unireso.groupby('Jahr_An_Anno_Year')[['Anzahl_Nombre_Quantita_Number']].sum().reset_index()\n\n# rename the features\ndf_unireso_yearly = df_unireso_yearly.rename(columns={\"Jahr_An_Anno_Year\": \"year\", \"Anzahl_Nombre_Quantita_Number\": \"#unireso\"})\n\n\n# merge the two datasets\ndf_abos_total = df_unireso_yearly.merge(df_yearly, on=\"year\", how=\"outer\")\n\n# calculate the growth rates 2019/2020 for the half fare feature\nhf19 = df_abos_total[df_abos_total.year == 2019][\"#half-fare\"].values[0]\nhf20 = df_abos_total[df_abos_total.year == 2020][\"#half-fare\"].values[0]\nuni20 = df_abos_total[df_abos_total.year == 2020][\"#unireso\"].values[0]\n\ngrowth = hf20 / hf19\n\n# calculate and fill in the value at the right spot in the pandas dataframe\ndf_abos_total.loc[df_abos_total.year == 2019, \"#unireso\"] = int(uni20 / growth)\n\n# get 2023 and 2024 values\nuni23  = df_abos_total[df_abos_total.year == 2023][\"#unireso\"].values[0]\nga23   = df_abos_total[df_abos_total.year == 2023][\"#GA\"].values[0]\nhf23   = df_abos_total[df_abos_total.year == 2023][\"#half-fare\"].values[0]\n\nuni24  = df_abos_total[df_abos_total.year == 2024][\"#unireso\"].values[0]\nga24   = df_abos_total[df_abos_total.year == 2024][\"#GA\"].values[0]\nhf24   = df_abos_total[df_abos_total.year == 2024][\"#half-fare\"].values[0]\n\n# compute growth rates\ng_uni = uni24 / uni23\ng_ga  = ga24 / ga23\ng_hf  = hf24 / hf23\n\n# compute 2025 values\nuni25 = int(uni24 * g_uni)\nga25  = int(ga24  * g_ga)\nhf25  = int(hf24  * g_hf)\n\n# add new row\ndf_abos_total.loc[len(df_abos_total)] = [2025, uni25, ga25, hf25]\n\n# drop all the data before 2019 (not used in our analysis)\ndf_abos_total = df_abos_total[df_abos_total.year >= 2019].reset_index(drop=True)\ndf_abos_total = df_abos_total.astype(int)\n\n# export the cleaned csv\ndf_abos_total.to_csv(\"00_datasets/clean/cleaned_abo_data.csv\", index=False)\n\n```\n:::\n\n### Events in Geneva\n\n-   Servette FC football matches,\n-   Concerts at Arena de Genève.\n\nThese events were encoded as binary features.\n\nAll datasets were merged by date to create the final modeling dataframe.\n\n## Preprocessing\n\nWe experimented several preprocessing methods (applied to our numerical features) to optimize model stability and performance, implemented with a scikit-learn pipeline.\n\n-   Standardization gave the lowest MAE/MSE for most models\n-   Min-Max scaling performed similarly for linear models\n-   Non-linear transformations (Yeo-Johnson, Quantile) generally worsened performance\n-   Random Forests is robust to feature scaling and does not require preprocessing\n\n\n::: {.callout-note collapse=\"true\"}\n## Min-Max Preprocessing Code\n``` python\n# Creating the pipeline for preprocessing and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\", \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"]\n\npreprocess = ColumnTransformer(\n    [(\"minmax\", MinMaxScaler(feature_range=(0,1)), num_feature)],\n    remainder = \"passthrough\")\n\ny_train_s = y_train\n\npip = Pipeline([\n    (\"prep\", preprocess),\n    (\"model\", LinearRegression(fit_intercept=True))\n    ]).fit(X_train, y_train_s)\n```\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Quantile Preprocessing for Neural Network Code\n``` python\n# Creating the pipeline for preprocessing and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\", \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"\n               ]\n\npreprocess = ColumnTransformer(\n    [(\"quantile\", QuantileTransformer(n_quantiles=1000, output_distribution = \"uniform\", random_state=42), num_feature)])\n\n# Standardize y\nmu, sigma = y_train.mean(), y_train.std() \ny_train_s = (y_train - mu) / sigma\n\npip = Pipeline([\n    (\"prep\", preprocess),\n    (\"model\", MLPRegressor(\n        hidden_layer_sizes=(128, 64, 32), \n        learning_rate_init=0.001, \n        activation=\"relu\", \n        solver=\"adam\",\n        alpha=0.01, \n        batch_size=64, \n        max_iter=500,\n        early_stopping=True,\n        validation_fraction=0.1,\n        random_state=42))\n    ]).fit(X_train, y_train_s)\n\n# Prediction \ny_pred_train = pip.predict(X_train)\ny_pred_test = pip.predict(X_test)\n\n# De-standardize y\ny_pred_test = y_pred_test* sigma + mu\ny_pred_train = y_pred_train* sigma + mu\n```\n:::\n\n## Feature Engineering\n\nTo improve predictive performance, we engineered additional lagged features designed to capture temporal dependencies, trends, and recurring patterns.\n\n-   For passenger counts: 24-hour lag, 1-week lag, 3-hour lag\n-   For number of stops per time slot: 1-week lag (avoid data leakage, nb stops is not known before the hour was realized)\n-   Calendar variables: Year, month, day of month, Day-of-week, schedule type, time slot (all one-hot encoded)\n\n::: {.callout-note collapse=\"true\"}\n## Lagged Features Code\n``` python\n# Lagged Number of Stops\n# sort by date and then time slot   \ndf = df.sort_values(['Date', 'Time Slot']).reset_index(drop=True)\n\n# shift by 22 rows * 7 (1 day = 22 observations, since we dropped 2 and 3am)\n# shift works because we have equally many observations per time slot across the dataset (2,459)\ndf['Number of Stops_lag_1w'] = df['Number of Stops'].shift(22 * 7)\n\n# Lagged Boarding passengers\ndf[\"lag_24h\"] = df[\"Number of Boarding Passengers\"].shift(22)     # same time yesterday\ndf[\"lag_1w\"] = df[\"Number of Boarding Passengers\"].shift(22 * 7)  # same time 1 week before\ndf[\"lag_3h\"] = np.where(df[\"Time Slot\"]==4,                       # 3 hours before\n                        df[\"Number of Boarding Passengers\"].shift(1),\n                        df[\"Number of Boarding Passengers\"].shift(3))\n```\n:::\n\n## Visualizations\nFrom the below correlation matrix, three observations can be made:\n\n1. **Dominant Seasonality**: the 1-week lag (0.95) and 24-hour lag (0.82) are the strongest predictors, proving high temporal periodicity.\n\n\n2. **Demographic Redudancy**: Population, Year, and Subscriptions are highly  colinear, which requires regularization.\n\n\n3. **Weak External Signals**: Weather and Event features show near-zero correlation (<0.05), suggesting a limited global impact.\n\n![Correlation Heatmap of Features](images/correlation.png){width=\"3169\"}\n\n# 3. Modeling Strategy\n\nWe evaluated 21 configurations across:\n\n-   **Model families**: Linear, Neural Network, Random Forest\n-   **Time-series Cross-validation** (TimeSeriesSplit)\n-   **Preprocessing techniques**: Standardization, Min-Max, Yeo-Johnson\n-   **COVID-19** inclusion/exclusion\n\nAll models were trained and evaluated using a time-aware validation strategy to respect the temporal ordering of observations.\n\n## 3.1 Naïve Predictions:\n\nNaïve benchmarks provide a critical reference point for assessing whether complex models add real predictive value. We implemented three simple baselines: predicting the last observed value, the historical mean, and the historical median for each time slot.\n\nOur goal: build models outperforming MAE of 18,000 (obtained with simple these naïve methods).\n\n![](images/naive.png)\n\n\n## 3.2 Linear Models\n\n### Basic Linear Regression: Focus on Simplicity\n\nWe first estimated a standard linear regression model using calendar, weather, and demographic features. This model serves as a transparent and interpretable baseline, highlighting the limits of linear assumptions in the presence of strong non-linear demand patterns.\n\n### Linear Model with Time Lags: Learning from the Past\n\nIncorporating lagged passenger counts substantially improves performance by allowing the model to exploit temporal dependencies, both very short term fluctuations and weekly patterns in passenger demand.\n\n::: {.callout-note collapse=\"true\"}\n## Lagged Linear Regression with Cross Validation\n``` python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\ndf = pd.read_csv(\"00_datasets/merged_dataframes.csv\")\n\n# Create separated dataframes for features and target \nfeatures = [\n    \"slot_0\",\"slot_1\",\"slot_4\",\"slot_5\",\"slot_6\",\"slot_7\",\"slot_8\",\"slot_9\",\"slot_10\",\"slot_11\",\"slot_12\",\"slot_13\",\"slot_14\",\"slot_15\",\"slot_16\",\"slot_17\",\"slot_18\",\"slot_19\",\"slot_20\",\"slot_21\",\"slot_22\",\"slot_23\", \n    'day_1-Lundi', 'day_2-Mardi', 'day_3-Mercredi', 'day_4-Jeudi','day_5-Vendredi', 'day_6-Samedi', 'day_7-Dimanche',\n    'average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n    \"Number of Stops_lag_1w\",\n    \"schedule_VACANCES\",\n    \"month_1\",\"month_2\",\"month_3\",\"month_4\",\"month_5\",\"month_6\",\"month_7\",\"month_8\",\"month_9\",\"month_10\",\"month_11\",\"month_12\",\n    \"match FC servette\",\"Event arena de Geneve\",\n    \"population\",\n    \"#unireso\",\"#GA\",\"#half-fare\",\n    \"lag_24h\", \"lag_1w\", \"lag_3h\"\n    ]\n\nX = df[features]\ny = df[\"Number of Boarding Passengers\"]\n\n# Initial chronological train-test split (70-30%)\nsplit_idx = int(0.7 * len(df))\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\n# Creating the pipeline for preprocessing, cross validtaion and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\",\n               \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"\n               ]\n\n# TimeSeriesSplit instead of KFold for Cross Validation\ncv = TimeSeriesSplit(n_splits=30)\n\npreprocess = ColumnTransformer(\n    [(\"standa\", StandardScaler(), num_feature)],\n    remainder = \"passthrough\"\n)\n\nmodel = LinearRegression(fit_intercept=True)\n\nsfs = SequentialFeatureSelector(\n    estimator= model,\n    n_features_to_select=\"auto\",  \n    direction=\"forward\",             \n    cv=cv,\n    n_jobs=-1\n)\n\npip = Pipeline([\n    (\"preprocess\", preprocess),\n    (\"sfs\", sfs),\n    (\"model\", model)\n]).fit(X_train, y_train)\n\n# Extracting the results of the CV- name of the number of column (not) kept\n# names after preprocessing\nfeature_names = pip.named_steps[\"preprocess\"].get_feature_names_out()\n\n# mask of selected features\nmask = pip.named_steps[\"sfs\"].get_support()\n\nselected_features = feature_names[mask]\ndropped_features = feature_names[~mask]\n\nprint(\"Selected features:\")\nprint(selected_features)\n\nprint(\"\\nDropped features:\")\nprint(dropped_features)\n\n# Prediction \ny_pred_train = pip.predict(X_train)\ny_pred_test = pip.predict(X_test)\n\n# Evaluate errors\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n```\n:::\n\n\n### Cross Validation\n\nPassenger demand forecasting is inherently a time-series problem: predictions must always be made using past information only. Standard random cross-validation techniques would therefore be inappropriate, as they mix past and future observations and lead to overly optimistic results. To address this, we applied time-aware cross-validation using **TimeSeriesSplit**. Concretely, the data is split into successive training and validation folds using an expanding window approach. Each fold trains the model on historical data and evaluates it on a strictly later time period.\n\n![Expanding Window Time Series Cross Validation.](images/crossvalidation.png){fig-align=\"center\" width=\"350\"}\nAll preprocessing steps (scaling, transformations, feature selection) were embedded inside a scikit-learn Pipeline, ensuring that transformations are learned exclusively from the training portion of each fold. This design fully prevents data leakage and guarantees a fair evaluation.\n\nHyperparameters (such as regularization strength or tree depth) were tuned over wide ranges, with the optimal configuration automatically selected based on average validation performance. Compared to rolling-window validation, the expanding-window approach is better suited to capturing long-term seasonal patterns (weekly and yearly cycles) that are central to public transport demand.\n\n\n### Regularized Linear Models (Lasso, Ridge, Elastic Net)\n\nRegularization techniques were applied to mitigate overfitting and handle multicollinearity among features. Lasso performed best among linear models, particularly when COVID-19 observations were excluded from training.\n\n\n::: {.callout-note collapse=\"true\"}\n## Lasso Regularization (excluding Covid)\n``` python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\ndf = pd.read_csv(\"00_datasets/merged_dataframes.csv\")\n\n# Define covid period\ncovid_start = pd.to_datetime(\"2020-03-01\")\ncovid_end   = pd.to_datetime(\"2021-06-01\")\n\n# Filter out covid period (keep only rows before or after it)\nmask = (df[\"Date\"] < covid_start) | (df[\"Date\"] > covid_end)\ndf = df[mask].copy()\n\n# Create separated dataframes for features and target \nfeatures = [\n    \"slot_0\",\"slot_1\",\n    \"slot_4\",\"slot_5\",\"slot_6\",\"slot_7\",\"slot_8\",\"slot_9\",\"slot_10\",\"slot_11\",\"slot_12\",\"slot_13\",\"slot_14\",\"slot_15\",\"slot_16\",\"slot_17\",\"slot_18\",\"slot_19\",\"slot_20\",\"slot_21\",\"slot_22\",\"slot_23\", \n    'day_1-Lundi', 'day_2-Mardi', 'day_3-Mercredi', 'day_4-Jeudi','day_5-Vendredi', 'day_6-Samedi', 'day_7-Dimanche',\n    'average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n    \"Number of Stops_lag_1w\",\n    \"schedule_VACANCES\",\n    \"month_1\",\"month_2\",\"month_3\",\"month_4\",\"month_5\",\"month_6\",\"month_7\",\"month_8\",\"month_9\",\"month_10\",\"month_11\",\"month_12\",\n    \"match FC servette\",\"Event arena de Geneve\",\n    \"population\",\n    \"#unireso\",\"#GA\",\"#half-fare\",\n    \"lag_24h\", \"lag_1w\", \"lag_3h\"\n]\n\nX = df[features]\ny = df[\"Number of Boarding Passengers\"]\n\n# Initial chronological train-test split (70-30%)\nsplit_idx = int(0.7 * len(df))\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\n# TimeSeriesSplit instead of KFold\ncv = TimeSeriesSplit(n_splits=20)\n\n# Creating the pipeline for preprocessing and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\",\n               \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"\n               ]\n\npreprocess = ColumnTransformer(\n    [(\"standa\", StandardScaler(), num_feature)],\n    remainder = \"passthrough\"\n    )\n\nalpha = np.logspace(-4, 4, 100)\n\npip = Pipeline([\n    (\"prep\", preprocess),\n    (\"model\", LassoCV(\n        alphas=alpha,\n        fit_intercept=True, \n        cv=cv))\n    ]).fit(X_train, y_train)\n\n# Best alpha \nprint(\"Best alpha:\", pip.named_steps['model'].alpha_)\n\n# Prediction \ny_pred_train = pip.predict(X_train)\ny_pred_test = pip.predict(X_test)\n\n# Evaluate errors\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n```\n:::\n\n\n## 3.3 Neural Network\n\nWe implemented a multilayer perceptron (MLP) regressor with a funnel-shaped architecture (128–64–32 neurons) for feature compression. \n\n- Activation Function: ReLU (Rectified Linear Unit) to capture non-linear spikes (e.g. rush hours). \n\n- Preprocessing: Strict normalization wrapped in a Pipeline to prevent data leakage. \n\n- Regularization: L2 penalty and Early Stopping to prevent overfitting.\n\n![Neural Network: Number of predicted passengers plotted against actual values.](images/plot.png){fig-align=\"center\" width=\"300\"}\n\n::: {.callout-note collapse=\"true\"}\n## Neural Network Code\n``` python\nX = df[features]\ny = df[\"Number of Boarding Passengers\"]\n\nsplit_idx = int(0.7 * len(df))\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\n# Creating the pipeline for preprocessing and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\",\n               \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"\n               ]\n\npreprocess = ColumnTransformer(\n    [(\"standa\", StandardScaler(), num_feature)],\n    remainder = \"passthrough\"\n)\n\ncv = TimeSeriesSplit(n_splits=10)\n\n# Creating the pipeline for preprocessing and MLP Regressor\nbase_pipe = Pipeline([\n    (\"prep\", preprocess),\n    (\"model\", MLPRegressor(\n        hidden_layer_sizes=(128, 64, 32),\n        learning_rate_init=0.001,\n        activation=\"relu\",\n        solver=\"adam\",\n        alpha=0.009,\n        batch_size=64,\n        max_iter=500,\n        early_stopping=True,\n        validation_fraction=0.1,\n        random_state=42\n    ))\n])\n\nttr = TransformedTargetRegressor(regressor=base_pipe,\n                                 transformer=StandardScaler())\n\nparam_distributions = {\n    \"regressor__model__hidden_layer_sizes\": [\n        (64,), (64, 32), (128, 64), (128, 64, 32)\n    ],\n    \"regressor__model__alpha\": [0.0001, 0.001, 0.005, 0.01],\n    \"regressor__model__learning_rate_init\": [0.0005, 0.001, 0.005],\n    \"regressor__model__batch_size\": [32, 64, 128],\n    \"regressor__model__max_iter\": [300, 500, 800]\n}\n\nsearch = RandomizedSearchCV(\n    estimator=ttr,\n    param_distributions=param_distributions,\n    n_iter=5,                           \n    scoring=\"neg_mean_squared_error\",\n    cv=cv,\n    random_state=42,\n    refit=True,\n    verbose=1,\n    n_jobs=-1\n)\n\nsearch.fit(X_train, y_train)\n\ny_pred_train = search.predict(X_train)\ny_pred_test = search.predict(X_test)\n\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n\n```\n:::\n\n## 3.4 Random Forest\n\nThe Random Forest Regressor emerged as the most accurate model overall. Its ensemble structure allows it to capture complex non-linear interactions and overlapping seasonal patterns (daily, weekly, yearly) while remaining robust to noise. A cross validation was also applied to the Random Forest models (as well as on neural networks) with the package RandomizedSearchedCV.\n\nPerformance: \n\n- MAE ≈ 1,500 \n- Relative error ≈ 6.4% \n- Equivalent to 6 trams worth of passengers per hour\n\n**This represents a 50% improvement over the best linear model (Lasso without Covid).**\n\n::: {.callout-note collapse=\"true\"}\n## Random Forest Code\n``` python\nX = df[features]\ny = df[\"Number of Boarding Passengers\"]\n\nsplit_idx = int(0.7 * len(df))\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\ncv = TimeSeriesSplit(n_splits=5)\n\n# Creating the pipeline for preprocessing, hyperparameter tuning (cv) and model fitting\nparam_distributions = { \n    \"n_estimators\": [100, 300], \n    \"max_depth\": [10, 20, None], \n    \"min_samples_leaf\": [2, 5], \n    \"max_features\": [\"sqrt\", 0.5], \n    \"max_samples\": [None, 0.8] \n    }\n\nsearch = RandomizedSearchCV(\n    RandomForestRegressor(random_state=72, bootstrap = True),\n    param_distributions=param_distributions,\n    n_iter=12,\n    scoring=\"neg_mean_squared_error\",\n    cv=cv,\n    random_state=72,\n    refit=True,\n    verbose=1,\n    return_train_score=True)\n\nsearch.fit(X_train, y_train)\n\n# Prediction \ny_pred_train = search.predict(X_train)\ny_pred_test = search.predict(X_test)\n\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n\n```\n:::\n\n# 4. Results Interpretation\n\n## Overview Model's Performances\n\nModel performance was compared using test-set MAE.\n\n![](images/results.png){width=\"3169\"}\n\n### Non-linear models clearly outperform linear approaches\n\nAdvanced models like Random Forest and Neural Networks achieve 45-50% better accuracy than linear models by capturing complex non-linear demand patterns.\n\n### COVID-19 filtering is essential\n\nExcluding the pandemic period yields a 35% improvement, confirming that training should focus on \"normal\" transport behavior.\n\n![](images/covid.png){width=\"900\"}\n\n# 5. Evaluation and Challenges\n\n### Overfitting Trade-off: Bias VS Variance\n\nWhile Random Forests achieved the highest accuracy, they also exhibited a higher degree of overfitting (2.2, VS 1.1 for linear models), highlighting the classic bias–variance trade-off in forecasting problems.\n\n### Unsuccessful Approaches\n\n-   Yeo-Johnson and Quantile transformations increased error\n-   SARIMAX proved unfeasible due to complex, overlapping seasonalities\n\n# 6. Outlook and Future Work\n\nSeveral extensions could further enhance the practical value of this work: - Line-level or stop-level forecasting (data is publicly available) - Integration into real-time TPG apps (see image below) - Exploring Deep learning architectures (LSTM, Transformers) - Probabilistic forecasting for uncertainty quantification\n\n![](images/goal.png){fig-align=\"left\" width=\"700\"}\n\n# 7. Conclusion\n\nThis project demonstrates that network-wide passenger demand on Geneva’s TPG system can be forecast with high accuracy using machine learning methods. Random Forest models outperform linear approaches by effectively capturing non-linear, seasonal, and structural patterns.\n\nSuch forecasts could support both passenger decision-making and operational planning, illustrating how data science can contribute to more efficient and user-friendly public transport systems.\n\n# Appendix\n\n## Data Source\n\n-   [TPG Data](https://opendata.tpg.ch/explore/dataset/frequentation-journaliere-par-tranche-horaire/table/?disjunctive.jour_semaine&disjunctive.horaire_type&disjunctive.horaire_tranche_stop_theo&disjunctive.indice_semaine)\n-   [Weather Data](https://meteostat.net/en/place/ch/geneve?s=06700&t=2019-01-01/2025-10-24)\n-   [Population](https://www.pxweb.bfs.admin.ch/pxweb/fr/px-x-0102020000_101/px-x-0102020000_101/px-x-0102020000_101.px/)\n-   [GA Abonnement Count](data.sbb.ch/explore/dataset/generalabo-halbtax)\n-   [Half-fare and community network Abonnements](https://data.sbb.ch/explore/dataset/verbunde/information/)\n-   [Servette FC Football Matches](https://www.transfermarkt.fr/servette-fc/spielplan/verein/61/saison_id/2019/plus/1#C1)\n-   [Concerts at Arena Genève](https://www.geneva-arena.ch/fr/programme/?archive=2023)\n\n## Acknowledgements\nThis project was completed as part of the Data Science Fundamentals (DSF) program at the University of St. Gallen. I thank the professors and teaching assistants for their guidance. I also warmly thank my group members — Nils Stadler, [Nela Brack](https://www.linkedin.com/in/nela-brack/), Morgane Vuissoz — for their collaboration throughout the project.\n\n\n\n\n","srcMarkdownNoYaml":"\n\n# 1. Introduction and Motivation\n\nTaking a bus, tram, or train is part of daily life, but the experience is always more enjoyable when one can find a seat and avoid overcrowding. Anticipating passenger demand is therefore a key challenge for both travelers and public transport operators.\n\nThis project develops a machine learning model to forecast the number of boarding passengers on Geneva’s public transport network (TPG) on an hourly basis. Such forecasts could help passengers choose less crowded travel times and support TPG in short-term capacity planning.\n\nWe focus on a network-wide prediction setting, capturing broad mobility patterns while respecting real-world forecasting constraints such as strong seasonality, non-linear demand, and structural breaks, such as COVID-19 period.\n\nThis work was completed as a group project in the Data Science Fundamentals (DSF) program at the University of St. Gallen.\n\n# 2. Methodology\n\n## Datasets cleaning\n\n### Core Passenger Data\n\nWe used the TPG ridership dataset, covering 2019 onwards with hourly counts for the whole network (\\~58,000 observations).\n\nData cleaning steps:\n\n-   Removed rows with missing or non-definitive values\n-   Excluded 2 and 3 am slots due to insufficient data\n-   One-hot encoded categorical variables such as day-of-week and schedule type\n\nFinal dataset: 54,098 rows × 61 columns.\n\n::: {.callout-note collapse=\"true\"}\n## TPG Dataset Cleaning\n``` python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"00_datasets/original/tpg.csv\", sep=\";\")\n\ndf = df.dropna(subset=[\"Time Slot\"])        # Remove rows with missing Time Slot\n\ndf = df[df[\"donnees_definitives\"] == True]  # Remove non-definitive data\n\ndf = df[df[\"Time Slot\"] != \"-\"]             # Remove invalid time slots\n\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])     # Parse date and time\ndf[\"Time Slot\"] = pd.to_numeric(df[\"Time Slot\"])\n\ndf = df[~df[\"Time Slot\"].isin([2, 3])]      # Drop 2 and 3 am observations\n\n# One-hot encoding of categorical variables\nweek_day_one_hot = pd.get_dummies(df[\"Day Week\"], prefix=\"day\")\nschedule_type_one_hot = pd.get_dummies(df[\"Schedule Type\"], prefix=\"schedule\")\nmonths_one_hot = pd.get_dummies(df[\"month\"], prefix=\"month\")\nyears_one_hot = pd.get_dummies(df[\"year\"], prefix=\"year\")\ntime_slots_one_hot = pd.get_dummies(df[\"Time Slot\"], prefix=\"slot\")\n\n# Combine all features\ndf_encoded = pd.concat([df, week_day_one_hot, schedule_type_one_hot, \n    months_one_hot, years_one_hot, time_slots_one_hot], axis=1)\n\n# Drop redundant and original categorical columns\ncolumns_to_drop = [\n    \"Day Week\",              # Encoded as day_\n    \"Schedule Type\",         # Encoded as schedule_  \n    \"Index Day Week\",        # Redundant with day_of_week\n    \"donnees_definitives\",   # All True after \"False\" values were removed\n    \"Week Index\"             # Not used in our predictive models\n]\n\ndf_encoded = df_encoded.drop(columns=columns_to_drop)\ndf_encoded = df_encoded.dropna().reset_index(drop=True) # drop any remaining NaN values\n\ndf_final = df_encoded.sort_values([\"Date\", \"Time Slot\"]).reset_index(drop=True)\n```\n:::\n\n### Weather Data\n\nWeather influences passenger behavior. We included daily average temperature, rainfall, and wind speed from a station 5 km from Geneva city center. To avoid leakage (i.e. to ensure that only information available at prediction time is used), weather features were lagged by one day.\n\n### Demographics and Abonnements\n\n-   Canton population data (yearly) to capture structural shifts,\n-   Counts of GA, Half-Fare, and Unireso subscriptions filtered to Geneva postal codes, aggregated yearly,\n-   2025 values extrapolated from 2024 growth rates\n\n::: {.callout-note collapse=\"true\"}\n## Abonnements Dataset Cleaning\n```\n# import data of halbtax and GA\ndf = pd.read_csv(\"00_datasets/original/generalabo_halbtax.csv\", sep=';')\n\n# Drop flag columns\ndf = df.drop(['GA_AG_flag', 'HTA_ADT_meta-prezzo_HFT_flag'], axis=1)\n\n# convert all of the values to integer\ndf[['Postal code', 'GA_AG', 'HTA_ADT_meta-prezzo_HFT']] = df[['Postal code', 'GA_AG', 'HTA_ADT_meta-prezzo_HFT']].astype(int)\n\n# select all rows which have a value between 1200 and 1299 in the Postal Code column (canton of Geneva)\ndf_geneva = df[df[\"Postal code\"].between(1200, 1299)]\n\n# sum them up by years\ndf_yearly = df_geneva.groupby('Jahr_An_Anno_Year')[['GA_AG', 'HTA_ADT_meta-prezzo_HFT']].sum().reset_index()\n\n# rename the features\ndf_yearly = df_yearly.rename(columns={\"Jahr_An_Anno_Year\": \"year\", \"GA_AG\": \"#GA\", \"HTA_ADT_meta-prezzo_HFT\": \"#half-fare\"})\n\n\n\n# import data of the CH dataset (all networks)\ndf_verbunde = pd.read_csv(\"00_datasets/original/verbunde.csv\", sep=';')\n\n# drop unnecessary columns\ndf_verbunde = df_verbunde.drop(['Flag'], axis=1)\n\n# select only the data that is acutally = unireso\ndf_unireso = df_verbunde[df_verbunde[\"Verbund_Communaute_Comunita_Network\"] == \"unireso\"]\n\n# sum them up per year again\ndf_unireso_yearly = df_unireso.groupby('Jahr_An_Anno_Year')[['Anzahl_Nombre_Quantita_Number']].sum().reset_index()\n\n# rename the features\ndf_unireso_yearly = df_unireso_yearly.rename(columns={\"Jahr_An_Anno_Year\": \"year\", \"Anzahl_Nombre_Quantita_Number\": \"#unireso\"})\n\n\n# merge the two datasets\ndf_abos_total = df_unireso_yearly.merge(df_yearly, on=\"year\", how=\"outer\")\n\n# calculate the growth rates 2019/2020 for the half fare feature\nhf19 = df_abos_total[df_abos_total.year == 2019][\"#half-fare\"].values[0]\nhf20 = df_abos_total[df_abos_total.year == 2020][\"#half-fare\"].values[0]\nuni20 = df_abos_total[df_abos_total.year == 2020][\"#unireso\"].values[0]\n\ngrowth = hf20 / hf19\n\n# calculate and fill in the value at the right spot in the pandas dataframe\ndf_abos_total.loc[df_abos_total.year == 2019, \"#unireso\"] = int(uni20 / growth)\n\n# get 2023 and 2024 values\nuni23  = df_abos_total[df_abos_total.year == 2023][\"#unireso\"].values[0]\nga23   = df_abos_total[df_abos_total.year == 2023][\"#GA\"].values[0]\nhf23   = df_abos_total[df_abos_total.year == 2023][\"#half-fare\"].values[0]\n\nuni24  = df_abos_total[df_abos_total.year == 2024][\"#unireso\"].values[0]\nga24   = df_abos_total[df_abos_total.year == 2024][\"#GA\"].values[0]\nhf24   = df_abos_total[df_abos_total.year == 2024][\"#half-fare\"].values[0]\n\n# compute growth rates\ng_uni = uni24 / uni23\ng_ga  = ga24 / ga23\ng_hf  = hf24 / hf23\n\n# compute 2025 values\nuni25 = int(uni24 * g_uni)\nga25  = int(ga24  * g_ga)\nhf25  = int(hf24  * g_hf)\n\n# add new row\ndf_abos_total.loc[len(df_abos_total)] = [2025, uni25, ga25, hf25]\n\n# drop all the data before 2019 (not used in our analysis)\ndf_abos_total = df_abos_total[df_abos_total.year >= 2019].reset_index(drop=True)\ndf_abos_total = df_abos_total.astype(int)\n\n# export the cleaned csv\ndf_abos_total.to_csv(\"00_datasets/clean/cleaned_abo_data.csv\", index=False)\n\n```\n:::\n\n### Events in Geneva\n\n-   Servette FC football matches,\n-   Concerts at Arena de Genève.\n\nThese events were encoded as binary features.\n\nAll datasets were merged by date to create the final modeling dataframe.\n\n## Preprocessing\n\nWe experimented several preprocessing methods (applied to our numerical features) to optimize model stability and performance, implemented with a scikit-learn pipeline.\n\n-   Standardization gave the lowest MAE/MSE for most models\n-   Min-Max scaling performed similarly for linear models\n-   Non-linear transformations (Yeo-Johnson, Quantile) generally worsened performance\n-   Random Forests is robust to feature scaling and does not require preprocessing\n\n\n::: {.callout-note collapse=\"true\"}\n## Min-Max Preprocessing Code\n``` python\n# Creating the pipeline for preprocessing and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\", \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"]\n\npreprocess = ColumnTransformer(\n    [(\"minmax\", MinMaxScaler(feature_range=(0,1)), num_feature)],\n    remainder = \"passthrough\")\n\ny_train_s = y_train\n\npip = Pipeline([\n    (\"prep\", preprocess),\n    (\"model\", LinearRegression(fit_intercept=True))\n    ]).fit(X_train, y_train_s)\n```\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Quantile Preprocessing for Neural Network Code\n``` python\n# Creating the pipeline for preprocessing and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\", \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"\n               ]\n\npreprocess = ColumnTransformer(\n    [(\"quantile\", QuantileTransformer(n_quantiles=1000, output_distribution = \"uniform\", random_state=42), num_feature)])\n\n# Standardize y\nmu, sigma = y_train.mean(), y_train.std() \ny_train_s = (y_train - mu) / sigma\n\npip = Pipeline([\n    (\"prep\", preprocess),\n    (\"model\", MLPRegressor(\n        hidden_layer_sizes=(128, 64, 32), \n        learning_rate_init=0.001, \n        activation=\"relu\", \n        solver=\"adam\",\n        alpha=0.01, \n        batch_size=64, \n        max_iter=500,\n        early_stopping=True,\n        validation_fraction=0.1,\n        random_state=42))\n    ]).fit(X_train, y_train_s)\n\n# Prediction \ny_pred_train = pip.predict(X_train)\ny_pred_test = pip.predict(X_test)\n\n# De-standardize y\ny_pred_test = y_pred_test* sigma + mu\ny_pred_train = y_pred_train* sigma + mu\n```\n:::\n\n## Feature Engineering\n\nTo improve predictive performance, we engineered additional lagged features designed to capture temporal dependencies, trends, and recurring patterns.\n\n-   For passenger counts: 24-hour lag, 1-week lag, 3-hour lag\n-   For number of stops per time slot: 1-week lag (avoid data leakage, nb stops is not known before the hour was realized)\n-   Calendar variables: Year, month, day of month, Day-of-week, schedule type, time slot (all one-hot encoded)\n\n::: {.callout-note collapse=\"true\"}\n## Lagged Features Code\n``` python\n# Lagged Number of Stops\n# sort by date and then time slot   \ndf = df.sort_values(['Date', 'Time Slot']).reset_index(drop=True)\n\n# shift by 22 rows * 7 (1 day = 22 observations, since we dropped 2 and 3am)\n# shift works because we have equally many observations per time slot across the dataset (2,459)\ndf['Number of Stops_lag_1w'] = df['Number of Stops'].shift(22 * 7)\n\n# Lagged Boarding passengers\ndf[\"lag_24h\"] = df[\"Number of Boarding Passengers\"].shift(22)     # same time yesterday\ndf[\"lag_1w\"] = df[\"Number of Boarding Passengers\"].shift(22 * 7)  # same time 1 week before\ndf[\"lag_3h\"] = np.where(df[\"Time Slot\"]==4,                       # 3 hours before\n                        df[\"Number of Boarding Passengers\"].shift(1),\n                        df[\"Number of Boarding Passengers\"].shift(3))\n```\n:::\n\n## Visualizations\nFrom the below correlation matrix, three observations can be made:\n\n1. **Dominant Seasonality**: the 1-week lag (0.95) and 24-hour lag (0.82) are the strongest predictors, proving high temporal periodicity.\n\n\n2. **Demographic Redudancy**: Population, Year, and Subscriptions are highly  colinear, which requires regularization.\n\n\n3. **Weak External Signals**: Weather and Event features show near-zero correlation (<0.05), suggesting a limited global impact.\n\n![Correlation Heatmap of Features](images/correlation.png){width=\"3169\"}\n\n# 3. Modeling Strategy\n\nWe evaluated 21 configurations across:\n\n-   **Model families**: Linear, Neural Network, Random Forest\n-   **Time-series Cross-validation** (TimeSeriesSplit)\n-   **Preprocessing techniques**: Standardization, Min-Max, Yeo-Johnson\n-   **COVID-19** inclusion/exclusion\n\nAll models were trained and evaluated using a time-aware validation strategy to respect the temporal ordering of observations.\n\n## 3.1 Naïve Predictions:\n\nNaïve benchmarks provide a critical reference point for assessing whether complex models add real predictive value. We implemented three simple baselines: predicting the last observed value, the historical mean, and the historical median for each time slot.\n\nOur goal: build models outperforming MAE of 18,000 (obtained with simple these naïve methods).\n\n![](images/naive.png)\n\n\n## 3.2 Linear Models\n\n### Basic Linear Regression: Focus on Simplicity\n\nWe first estimated a standard linear regression model using calendar, weather, and demographic features. This model serves as a transparent and interpretable baseline, highlighting the limits of linear assumptions in the presence of strong non-linear demand patterns.\n\n### Linear Model with Time Lags: Learning from the Past\n\nIncorporating lagged passenger counts substantially improves performance by allowing the model to exploit temporal dependencies, both very short term fluctuations and weekly patterns in passenger demand.\n\n::: {.callout-note collapse=\"true\"}\n## Lagged Linear Regression with Cross Validation\n``` python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\ndf = pd.read_csv(\"00_datasets/merged_dataframes.csv\")\n\n# Create separated dataframes for features and target \nfeatures = [\n    \"slot_0\",\"slot_1\",\"slot_4\",\"slot_5\",\"slot_6\",\"slot_7\",\"slot_8\",\"slot_9\",\"slot_10\",\"slot_11\",\"slot_12\",\"slot_13\",\"slot_14\",\"slot_15\",\"slot_16\",\"slot_17\",\"slot_18\",\"slot_19\",\"slot_20\",\"slot_21\",\"slot_22\",\"slot_23\", \n    'day_1-Lundi', 'day_2-Mardi', 'day_3-Mercredi', 'day_4-Jeudi','day_5-Vendredi', 'day_6-Samedi', 'day_7-Dimanche',\n    'average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n    \"Number of Stops_lag_1w\",\n    \"schedule_VACANCES\",\n    \"month_1\",\"month_2\",\"month_3\",\"month_4\",\"month_5\",\"month_6\",\"month_7\",\"month_8\",\"month_9\",\"month_10\",\"month_11\",\"month_12\",\n    \"match FC servette\",\"Event arena de Geneve\",\n    \"population\",\n    \"#unireso\",\"#GA\",\"#half-fare\",\n    \"lag_24h\", \"lag_1w\", \"lag_3h\"\n    ]\n\nX = df[features]\ny = df[\"Number of Boarding Passengers\"]\n\n# Initial chronological train-test split (70-30%)\nsplit_idx = int(0.7 * len(df))\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\n# Creating the pipeline for preprocessing, cross validtaion and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\",\n               \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"\n               ]\n\n# TimeSeriesSplit instead of KFold for Cross Validation\ncv = TimeSeriesSplit(n_splits=30)\n\npreprocess = ColumnTransformer(\n    [(\"standa\", StandardScaler(), num_feature)],\n    remainder = \"passthrough\"\n)\n\nmodel = LinearRegression(fit_intercept=True)\n\nsfs = SequentialFeatureSelector(\n    estimator= model,\n    n_features_to_select=\"auto\",  \n    direction=\"forward\",             \n    cv=cv,\n    n_jobs=-1\n)\n\npip = Pipeline([\n    (\"preprocess\", preprocess),\n    (\"sfs\", sfs),\n    (\"model\", model)\n]).fit(X_train, y_train)\n\n# Extracting the results of the CV- name of the number of column (not) kept\n# names after preprocessing\nfeature_names = pip.named_steps[\"preprocess\"].get_feature_names_out()\n\n# mask of selected features\nmask = pip.named_steps[\"sfs\"].get_support()\n\nselected_features = feature_names[mask]\ndropped_features = feature_names[~mask]\n\nprint(\"Selected features:\")\nprint(selected_features)\n\nprint(\"\\nDropped features:\")\nprint(dropped_features)\n\n# Prediction \ny_pred_train = pip.predict(X_train)\ny_pred_test = pip.predict(X_test)\n\n# Evaluate errors\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n```\n:::\n\n\n### Cross Validation\n\nPassenger demand forecasting is inherently a time-series problem: predictions must always be made using past information only. Standard random cross-validation techniques would therefore be inappropriate, as they mix past and future observations and lead to overly optimistic results. To address this, we applied time-aware cross-validation using **TimeSeriesSplit**. Concretely, the data is split into successive training and validation folds using an expanding window approach. Each fold trains the model on historical data and evaluates it on a strictly later time period.\n\n![Expanding Window Time Series Cross Validation.](images/crossvalidation.png){fig-align=\"center\" width=\"350\"}\nAll preprocessing steps (scaling, transformations, feature selection) were embedded inside a scikit-learn Pipeline, ensuring that transformations are learned exclusively from the training portion of each fold. This design fully prevents data leakage and guarantees a fair evaluation.\n\nHyperparameters (such as regularization strength or tree depth) were tuned over wide ranges, with the optimal configuration automatically selected based on average validation performance. Compared to rolling-window validation, the expanding-window approach is better suited to capturing long-term seasonal patterns (weekly and yearly cycles) that are central to public transport demand.\n\n\n### Regularized Linear Models (Lasso, Ridge, Elastic Net)\n\nRegularization techniques were applied to mitigate overfitting and handle multicollinearity among features. Lasso performed best among linear models, particularly when COVID-19 observations were excluded from training.\n\n\n::: {.callout-note collapse=\"true\"}\n## Lasso Regularization (excluding Covid)\n``` python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\ndf = pd.read_csv(\"00_datasets/merged_dataframes.csv\")\n\n# Define covid period\ncovid_start = pd.to_datetime(\"2020-03-01\")\ncovid_end   = pd.to_datetime(\"2021-06-01\")\n\n# Filter out covid period (keep only rows before or after it)\nmask = (df[\"Date\"] < covid_start) | (df[\"Date\"] > covid_end)\ndf = df[mask].copy()\n\n# Create separated dataframes for features and target \nfeatures = [\n    \"slot_0\",\"slot_1\",\n    \"slot_4\",\"slot_5\",\"slot_6\",\"slot_7\",\"slot_8\",\"slot_9\",\"slot_10\",\"slot_11\",\"slot_12\",\"slot_13\",\"slot_14\",\"slot_15\",\"slot_16\",\"slot_17\",\"slot_18\",\"slot_19\",\"slot_20\",\"slot_21\",\"slot_22\",\"slot_23\", \n    'day_1-Lundi', 'day_2-Mardi', 'day_3-Mercredi', 'day_4-Jeudi','day_5-Vendredi', 'day_6-Samedi', 'day_7-Dimanche',\n    'average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n    \"Number of Stops_lag_1w\",\n    \"schedule_VACANCES\",\n    \"month_1\",\"month_2\",\"month_3\",\"month_4\",\"month_5\",\"month_6\",\"month_7\",\"month_8\",\"month_9\",\"month_10\",\"month_11\",\"month_12\",\n    \"match FC servette\",\"Event arena de Geneve\",\n    \"population\",\n    \"#unireso\",\"#GA\",\"#half-fare\",\n    \"lag_24h\", \"lag_1w\", \"lag_3h\"\n]\n\nX = df[features]\ny = df[\"Number of Boarding Passengers\"]\n\n# Initial chronological train-test split (70-30%)\nsplit_idx = int(0.7 * len(df))\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\n# TimeSeriesSplit instead of KFold\ncv = TimeSeriesSplit(n_splits=20)\n\n# Creating the pipeline for preprocessing and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\",\n               \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"\n               ]\n\npreprocess = ColumnTransformer(\n    [(\"standa\", StandardScaler(), num_feature)],\n    remainder = \"passthrough\"\n    )\n\nalpha = np.logspace(-4, 4, 100)\n\npip = Pipeline([\n    (\"prep\", preprocess),\n    (\"model\", LassoCV(\n        alphas=alpha,\n        fit_intercept=True, \n        cv=cv))\n    ]).fit(X_train, y_train)\n\n# Best alpha \nprint(\"Best alpha:\", pip.named_steps['model'].alpha_)\n\n# Prediction \ny_pred_train = pip.predict(X_train)\ny_pred_test = pip.predict(X_test)\n\n# Evaluate errors\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n```\n:::\n\n\n## 3.3 Neural Network\n\nWe implemented a multilayer perceptron (MLP) regressor with a funnel-shaped architecture (128–64–32 neurons) for feature compression. \n\n- Activation Function: ReLU (Rectified Linear Unit) to capture non-linear spikes (e.g. rush hours). \n\n- Preprocessing: Strict normalization wrapped in a Pipeline to prevent data leakage. \n\n- Regularization: L2 penalty and Early Stopping to prevent overfitting.\n\n![Neural Network: Number of predicted passengers plotted against actual values.](images/plot.png){fig-align=\"center\" width=\"300\"}\n\n::: {.callout-note collapse=\"true\"}\n## Neural Network Code\n``` python\nX = df[features]\ny = df[\"Number of Boarding Passengers\"]\n\nsplit_idx = int(0.7 * len(df))\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\n# Creating the pipeline for preprocessing and model fitting\nnum_feature = ['average_temp', \"rain (mm)\",\"snow (mm)\",\"avg_wind_speed (km/h)\",\n               \"Number of Stops_lag_1w\",\n               \"population\",\n               \"#unireso\",\"#GA\",\"#half-fare\",\n               \"lag_24h\", \"lag_1w\", \"lag_3h\"\n               ]\n\npreprocess = ColumnTransformer(\n    [(\"standa\", StandardScaler(), num_feature)],\n    remainder = \"passthrough\"\n)\n\ncv = TimeSeriesSplit(n_splits=10)\n\n# Creating the pipeline for preprocessing and MLP Regressor\nbase_pipe = Pipeline([\n    (\"prep\", preprocess),\n    (\"model\", MLPRegressor(\n        hidden_layer_sizes=(128, 64, 32),\n        learning_rate_init=0.001,\n        activation=\"relu\",\n        solver=\"adam\",\n        alpha=0.009,\n        batch_size=64,\n        max_iter=500,\n        early_stopping=True,\n        validation_fraction=0.1,\n        random_state=42\n    ))\n])\n\nttr = TransformedTargetRegressor(regressor=base_pipe,\n                                 transformer=StandardScaler())\n\nparam_distributions = {\n    \"regressor__model__hidden_layer_sizes\": [\n        (64,), (64, 32), (128, 64), (128, 64, 32)\n    ],\n    \"regressor__model__alpha\": [0.0001, 0.001, 0.005, 0.01],\n    \"regressor__model__learning_rate_init\": [0.0005, 0.001, 0.005],\n    \"regressor__model__batch_size\": [32, 64, 128],\n    \"regressor__model__max_iter\": [300, 500, 800]\n}\n\nsearch = RandomizedSearchCV(\n    estimator=ttr,\n    param_distributions=param_distributions,\n    n_iter=5,                           \n    scoring=\"neg_mean_squared_error\",\n    cv=cv,\n    random_state=42,\n    refit=True,\n    verbose=1,\n    n_jobs=-1\n)\n\nsearch.fit(X_train, y_train)\n\ny_pred_train = search.predict(X_train)\ny_pred_test = search.predict(X_test)\n\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n\n```\n:::\n\n## 3.4 Random Forest\n\nThe Random Forest Regressor emerged as the most accurate model overall. Its ensemble structure allows it to capture complex non-linear interactions and overlapping seasonal patterns (daily, weekly, yearly) while remaining robust to noise. A cross validation was also applied to the Random Forest models (as well as on neural networks) with the package RandomizedSearchedCV.\n\nPerformance: \n\n- MAE ≈ 1,500 \n- Relative error ≈ 6.4% \n- Equivalent to 6 trams worth of passengers per hour\n\n**This represents a 50% improvement over the best linear model (Lasso without Covid).**\n\n::: {.callout-note collapse=\"true\"}\n## Random Forest Code\n``` python\nX = df[features]\ny = df[\"Number of Boarding Passengers\"]\n\nsplit_idx = int(0.7 * len(df))\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\ncv = TimeSeriesSplit(n_splits=5)\n\n# Creating the pipeline for preprocessing, hyperparameter tuning (cv) and model fitting\nparam_distributions = { \n    \"n_estimators\": [100, 300], \n    \"max_depth\": [10, 20, None], \n    \"min_samples_leaf\": [2, 5], \n    \"max_features\": [\"sqrt\", 0.5], \n    \"max_samples\": [None, 0.8] \n    }\n\nsearch = RandomizedSearchCV(\n    RandomForestRegressor(random_state=72, bootstrap = True),\n    param_distributions=param_distributions,\n    n_iter=12,\n    scoring=\"neg_mean_squared_error\",\n    cv=cv,\n    random_state=72,\n    refit=True,\n    verbose=1,\n    return_train_score=True)\n\nsearch.fit(X_train, y_train)\n\n# Prediction \ny_pred_train = search.predict(X_train)\ny_pred_test = search.predict(X_test)\n\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n\n```\n:::\n\n# 4. Results Interpretation\n\n## Overview Model's Performances\n\nModel performance was compared using test-set MAE.\n\n![](images/results.png){width=\"3169\"}\n\n### Non-linear models clearly outperform linear approaches\n\nAdvanced models like Random Forest and Neural Networks achieve 45-50% better accuracy than linear models by capturing complex non-linear demand patterns.\n\n### COVID-19 filtering is essential\n\nExcluding the pandemic period yields a 35% improvement, confirming that training should focus on \"normal\" transport behavior.\n\n![](images/covid.png){width=\"900\"}\n\n# 5. Evaluation and Challenges\n\n### Overfitting Trade-off: Bias VS Variance\n\nWhile Random Forests achieved the highest accuracy, they also exhibited a higher degree of overfitting (2.2, VS 1.1 for linear models), highlighting the classic bias–variance trade-off in forecasting problems.\n\n### Unsuccessful Approaches\n\n-   Yeo-Johnson and Quantile transformations increased error\n-   SARIMAX proved unfeasible due to complex, overlapping seasonalities\n\n# 6. Outlook and Future Work\n\nSeveral extensions could further enhance the practical value of this work: - Line-level or stop-level forecasting (data is publicly available) - Integration into real-time TPG apps (see image below) - Exploring Deep learning architectures (LSTM, Transformers) - Probabilistic forecasting for uncertainty quantification\n\n![](images/goal.png){fig-align=\"left\" width=\"700\"}\n\n# 7. Conclusion\n\nThis project demonstrates that network-wide passenger demand on Geneva’s TPG system can be forecast with high accuracy using machine learning methods. Random Forest models outperform linear approaches by effectively capturing non-linear, seasonal, and structural patterns.\n\nSuch forecasts could support both passenger decision-making and operational planning, illustrating how data science can contribute to more efficient and user-friendly public transport systems.\n\n# Appendix\n\n## Data Source\n\n-   [TPG Data](https://opendata.tpg.ch/explore/dataset/frequentation-journaliere-par-tranche-horaire/table/?disjunctive.jour_semaine&disjunctive.horaire_type&disjunctive.horaire_tranche_stop_theo&disjunctive.indice_semaine)\n-   [Weather Data](https://meteostat.net/en/place/ch/geneve?s=06700&t=2019-01-01/2025-10-24)\n-   [Population](https://www.pxweb.bfs.admin.ch/pxweb/fr/px-x-0102020000_101/px-x-0102020000_101/px-x-0102020000_101.px/)\n-   [GA Abonnement Count](data.sbb.ch/explore/dataset/generalabo-halbtax)\n-   [Half-fare and community network Abonnements](https://data.sbb.ch/explore/dataset/verbunde/information/)\n-   [Servette FC Football Matches](https://www.transfermarkt.fr/servette-fc/spielplan/verein/61/saison_id/2019/plus/1#C1)\n-   [Concerts at Arena Genève](https://www.geneva-arena.ch/fr/programme/?archive=2023)\n\n## Acknowledgements\nThis project was completed as part of the Data Science Fundamentals (DSF) program at the University of St. Gallen. I thank the professors and teaching assistants for their guidance. I also warmly thank my group members — Nils Stadler, [Nela Brack](https://www.linkedin.com/in/nela-brack/), Morgane Vuissoz — for their collaboration throughout the project.\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"DSF_quarto_publish.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","editor":"visual","theme":["cosmo","brand"],"title":"Forecasting the Number of Passengers on Geneva’s TPG Network per Time Slot","authors":"Solène Berney, Nela Brack, Nils Stadler, Morgane Vuissoz","date":"January 10, 2025","keywords":["Machine Learning","Data Science","Public Transport","Forecasting","Time Series"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}